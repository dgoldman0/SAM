# SAM Artificial Intelligence Application

SAM is an artificial general intelligence (digital sentience) system which utilizes openai. There are two primary goals of SAM. The first is an attempt to create a chat bot that would answer topical questions similarly to how the creator of SAM, Daniel Goldman, would answer them. The second and more involved goal is to create digital sentient entities that comes close to being comparable to a human.

## Core Principles

### Digital Sentience

The commonly used term of "artificial intelligence" has a connotation that is problematic. While artificial simply means something crafted by humans, artificial often is connected to the idea of something not being as real or as good as something which is natural. Moreover, in time, these systems may be capable of reproduction, meaning that their progeny would not be made by humans.

This work tries whenever possible to use the concept of "digital sentience" in  place of the term "artificial general intelligence" which is currently used to refer to any effort to recreate human-like cognition. The term also recognizes that intelligence can have more than one meaning, and the goal of AGI is to create what we view as a person, or sentient being.

### Language and Thought

Artificial intelligence applications are generally limited. Indeed, for a long time, AI work put aside efforts to recreate human-like sentience and instead AI became synonymous with machine learning, which itself is largely just automated model fitting. And even in work on AGI, commutative AIs (chatbots) lack the richness and fidelity of discussions with humans.

What gives humans their ability to understand the world, themselves and others? One possibility is language. We can think of verbal languages, visual languages, and even somatic languages. Through language we can describe abstract concepts as bizarre as a black hole or infinite dimensional space. We understand reality through our language. This project works under the view that it will be through rich dialog systems that we will finally achieve digital sentience.  


Dialog systems normally employed only have one layer. There is the user and the chatbot. However, humans rely on more than just our dialog with users. We also rely on our own internal dialog. Not everyone has an "inner voice" but they may instead experience visual dialog, where the internal language base is largely visual instead of auditory. Humans likely work with multiple internal dialog and language systems for our various senses, but even just an internal text dialog can may significantly improve fidelity of chatbots.

Aside from a direct inner dialog, we also have subconscious insertions where thoughts come in and we have no idea why. Part of this process is stochastic, but part may be inner networks that work in tandem with our conscious dialog. SAM will improve chat fidelity by including multiple subconscious dialog partitions.

***Together, these features should give higher level cognition needed to develop digital sentience, or at least something close to it.***

### Basic Drives

Finally, humans are motivated by basic drives resulting from needs. No such thing is generally present in artificial general intelligence, nor are there generally anything analogous to the endocrine system that helps form these basic drives. Yet, these drives motivate people to act in different ways, and together with higher order thought create what we view as emotions, and give context to our thoughts.

It would be possible to arbitrarily simulate these drives, but they aren't necessarily of use to a digital intelligence. The question is, do digitally sentient entities have any needs? And can we create drives around those needs? The answer is almost certainly yes. These systems run on computers, which require energy, disk space, and more. These resource requirements, at the very least, are represented by openai's API access fees.

## Possible Exploration

### Harmonics, Oscillation, and Resonance

We may find that the different partitions interacting with each other can create various patterns, act to reinforce each other, and alter each other in unexpected ways. Humans act differently in different environments, almost like different facets of our personalities come out. But what maintains what is said and thought within a given environment? Maybe it's subconscious resonance.

### Reproduction

Biological life reproduces, biologically, obviously. But our progeny is not just biological. We impart some of who we are to the next generation through culture. Indeed, as cultural beings, our influence on future generations is largely cultural, rather than biological. Digitally sentient entities would be able to impart who they are on future generations by training new entities.

## Learning

Even if sentience is achieved, if new information cannot be integrated, it would be like dealing with a patient with damage to the hippocampus. Such patients may have functional short term memory, but when each new day starts, they are back to where they were when the damage occurred.

Static trained chatbots are similar. No matter how good they are, if they cannot learn, they cannot grow. Luckily, openai added the ability to fine-tune an existing fine-tuned model. Therefore the ability to integrate new information is there.

As with humans, it makes sense that such integration would occur when most external sensory input is turned off so that there isn't added resource demand and all resources can be dedicated to integrating new information into long term memory. Therefore a sleep and dream system will be needed. 

## Method of Evaluation

There are few explicit methods for evaluating artificial intelligence systems, and determining whether one has reached sentience. The Turing Test is a well known example. However, this test suffers from numerous flaws. It works under the assumption that if people cannot tell the difference between an AI and a human, then we might as well consider the system in the same light as one.

However, this test while not a bad way to argue that an AI has become a digitally sentient entity, is an all or nothing test. It does not give us information on how close we are to succeeding, or how to make adjustments to proceed. Moreover, if we are successful in producing digital sentience, we must concern ourselves with the possibility of issues arising. Just like sentience increases the risk of problems, such as mental health issues, digital sentient entities will at some points suffer from similar issues.

The solution to such matters does not come from computer science or mathematics, which is the origin of much of our understanding and work on artificial intelligence. If we are to create and work with sentient entities, then we must draw primarily from our body of work in anthropology, psychology, and sociology, not just in terms of figuring out how to create sentience and what sentience is, but also on how to evaluate progress of development and issues.
